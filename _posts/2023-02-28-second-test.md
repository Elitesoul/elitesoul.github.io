# Ⅵ. big-O

- big-O 시간 : 알고리즘의 효율성을 나타내는 지표 혹은 언어

# 시간 복잡도

- 점근적 실행 시간 (Asymptotic runtime), 또는 big-O 시간에 대한 개념
- 가장 흔하게 사용되는 것 : $O(\log{N}), O(N\log{N}), O(N), (N^{2}), O(2^{N})$
- 실행 시간에 다양한 변수가 포함될 수도 있다

## 데이터 전송 ‘알고리즘’의 실행 시간에 대한 설명

- 온라인 전송 : O(s), s는 파일의 크기
- 비행기를 통한 전송 : 파일 크기에 관계 없이 O(1), 상수 시간만큼 소요
- 선형식은 언젠가 상수를 뛰어넘게 된다

## big-O, big-Θ, big-Ω

- O (big-O) : 시간의 상한
    - 알고리즘 수행 시간의 상한이 되고, ‘작거나 같은’ 부등호와도 비슷한 관계가 있다
- Ω (big-Omega) : 등가 개념 혹은 하한
    - 해당 알고리즘은 Ω 수행 시간보다 빠를 수 없다
- Θ (big-Theta) : 위의 둘 다 의미
- (면접에서는) Θ와 O를 하나로 합쳐 표현하려는 것 같다
- 수행 시간은 언제나 딱 맞게 표기하려 할 것이다

## 최선의 경우, 최악의 경우, 평균적인 경우

### 퀵 정렬 (Quick Sort)의 관점

- 최선의 경우 : 모든 원소가 동일한 경우 → 단순히 배열을 한 차례 순회하고 끝
    - O(N)
- 최악의 경우 : 가장 큰 원소가 계속해서 축이 되는 경우
    - $O(N^{2})$
- 평균적인 경우
    - $O(Nlog{N})$

### 최선의/최악의/평균적인 경우와 big-O/Θ/Ω 사이의 관련성

- 최선의, 최악의, 평균의 경우는 어떤 입력 혹은 상황에 대해서 big-O(혹은 big-Theta) 시간으로 설명한다
- big-O, big-Omega, big-Theta는 각각 상한, 하한, 딱 맞는 수행시간을 의미한다

# 공간 복잡도

- 알고리즘에서는 시간 뿐 아니라 메모리(혹은 공간) 또한 신경 써야 한다
- 시간 복잡도와 평행선을 달리는 개념
- 재귀 호출에서 사용하는 스택 공간 또한 공간 복잡도 계산에 포함된다
- 단지 n번 호출했다고 해서 O(n) 공간을 사용한다고 말할 수는 없다

# 상수항은 무시하라

- big-O는 단순히 증가하는 비율을 나타내는 개념
    
    → 수행 시간에서 상수항을 무시한다
    
- 단지 O(N)이 언제나 O(2N)보다 나은 것이 아니라는 사실만 받아들이면 된다

# 지배적이지 않은 항은 무시하라

- 수식에서 지배적이지 않은 항을 무시해도 된다
    - $O(N^{2}+N)은 O(N^{2})$이 된다
    - $O(N+\log{N})은 O(N)$이 된다
    - $O(5*2^{N}+1000N^{100})은 O(2^{N})$이 된다
- 각 변수 사이에 존재하는 특별한 관계를 알 수 없는 경우 합이 남아 있을 수 있다
    - $O(B^{2}+A)$

## 흔히 사용되는 big-O 시간의 증가율 그래프

![Untitled](Untitled.png)

# 여러 부분으로 이루어진 알고리즘 : 덧셈 vs. 곱셈

- “A 일을 모두 끝마친 후에 B 일을 수행하라”의 형태
    
    → A와 B의 수행 시간을 더해야 한다
    
    ```python
    덧셈 수행 시간: O(A + B) -> A의 일을 한 뒤에 B의 일을 수행
    for(int a: arrA) {
    	print(a);
    }
    for(int b: arrB) {
    	print(b);
    }
    ```
    
- “A 일을 할 때마다 B 일을 수행하라”의 형태
    
    → A와 B의 수행 시간을 곱해야 한다
    
    ```python
    곱셈 수행 시간: O(A * B) -> A의 각 원소에 대해 B의 일을 수행
    for(int a: arrA) {
    	for(int b: arrB) {
    		print(a + ", " + b);
    	}
    }
    ```
    

# 상환 시간

- ArrayList (동적 가변크기 배열) : 배열의 역할을 함과 동시에 크기가 자유롭게 조절되는 자료구조
    - 배열로 구현
    - 배열의 용량이 꽉 찼을 때, 기존 보다 크기가 두 배 큰 배열을 만든 뒤, 이전 배열의 모든 원소를 새 배열로 복사
        - 삽입 연산의 수행 시간
            - 배열이 가득 차 있는 경우 : O(N) → 기존의 모든 원소를 새 배열로 복사하기 때문
            - 대다수의 경우 가용 공간이 존재하기 때문에 O(1)
- 상환 시간 : 최악의 경우는 가끔 발생하지만 한 번 발생하면 그 후로 꽤 오랫동안 나타나지 않으므로 비용(수행 시간)을 분할 상환한다는 개념
    - X개의 원소를 삽입했을 때 필요한 시간 : O(2X)
        
        → 분할 상환해보면 삽입 한 번에 필요한 시간은 O(1)
        

# log N 수행 시간

- 이진 탐색의 경우 (binary search)
- $2^{k}=N$을 만족하는 k는 무엇인가? → 로그(log)
    
    $$
    2^{4}=16\to \log_2{16}=4 \\ \log_2{N}=k \to 2^{k}=N
    $$
    
- 원소의 개수가 절반씩 줄어드는 경우 → O(logN)이 될 가능성이 크다
- big-O에선 로그의 밑을 고려할 필요가 없다고 보면 된다

# 재귀적으로 수행 시간 구하기

```java
int f(int n) {
	if(n <= 1) {
		return 1;
	}
	return f(n - 1) + f(n - 1);
}
```

- 함수 f가 두 번 호출된 것을 보고 많은 사람들이 성급하게 $O(N^{2})$이라고 결론 내릴 것이다 → **완전 틀렸다**
- 트리의 깊이가 N이고, 각 노드(함수 호출)는 두 개의 자식 노드를 갖고 있다
    
    → 한 단계 깊어질 때마다 이전보다 두 배 더 많이 호출된다
    
    | 깊이 | 노드의 개수 | 다른 표현 방식 |
    | --- | --- | --- |
    | 0 | 1 | 2^0 |
    | 1 | 2 | 2^1 |
    | 2 | 4 | 2^2 |
    | 3 | 8 | 2^3 |
    - 전체 노드의 개수 : $2^0 +2^1+2^2+2^3+···+2^{N}=2^{N+1}-1$
- 다수의 호출로 이루어진 재귀 함수에선 수행 시간은 보통 $O(분기^{깊이})$로 표현되곤 한다
    - 분기 (Branch) : 재귀 함수가 자신을 재호출하는 횟수
    
    → 수행 시간 : $O(2^{N})$
    
- 지수항의 밑은 무시하면 안 된다
- 공간 복잡도 : O(N) → 특정 시각에 사용되는 공간의 크기가 O(N)이기 때문

# 예제 및 연습 문제

- 책을 보면서 풀어봅시다

## 예제 1

- 아래 코드의 시간 복잡도? → O(N), 배열을 여러번 읽어도 영향이 없다

```java
public static void foo(int[] array) {
		int sum = 0;
		int product = 1;
		for (int i = 0; i < array.length; i++) {
			sum += array[i];
		}
		for (int i = 0; i < array.length; i++) {
			product *= array[i];
		}	
		System.out.println(sum + ", " + product);
}
```

## 예제 2

- 아래 코드의 시간 복잡도?

```java
public static void printPairs(int[] array) {
		for (int i = 0; i < array.length; i++) {
			for (int j = 0; j < array.length; j++) {
				System.out.println(array[i] + "," + array[j]);
			}
		}
}
```

- 안쪽 루프의 반복 횟수는 O(N)이고 이 루프가 N번 반복 → $O(N^{2})$
- 위 코드가 ‘무엇을 의미하는지’ 살펴봄으로써 구할 수도 있다
    
    → 모든 쌍을 출력하는 코드, 두 원소의 쌍이 N^2 개
    

## 예제 3

- 아래 코드의 시간 복잡도?

```java
public static void printUnorderedPairs(int[] array) {
	for (int i = 0; i < array.length; i++) {
		for (int j = i; j < array.length; j++) {
			System.out.println(array[i] + "," + array[j]);
		}
	}
}
```

### 반복 횟수 세어보기

- j가 N - 1 부터 1까지 반복 횟수가 줄어들면서 반복된다
- N(N - 1) / 2 → $O(N^{2})$

### 코드가 무엇을 의미하는가

- j 가 i 보다 큰 모든 (i, j) 쌍을 반복하고 있다
- 모든 쌍의 개수 : $N^2$ → $O(N^2)$sss

### 평균을 이용한 방법

- 반복 횟수의 평균을 구해볼 수 있다
- 바깥 루프가 N번 반복되고 안쪽 루프가 평균 N / 2 번 반복된다

## 예제 4

- 아래 코드의 시간 복잡도?

```java
void printUnorderedPairs(int[] arrayA, int[] arrayB) {
    for(int i = 0; i < arrayA.length; i++) {
        for(int j = 0; j < arrayB.length; j++) {
            if(arrayA[i] < arrayB[j]) {
                System.out.println(arrayA[i] + ", " + arrayB[j]);
            }
        }
    }
}
```

- arrayA의 길이 : a , arrayB의 길이 : b 일 때
    
    → arrayA의 원소 하나당 안쪽 루프는 b번 반복된다
    
    → 따라서, O(ab)가 된다
    

## 예제 7

- 다음 중 O(N)과 같은 것들?
    - O(N + P), P < N / 2일 때
        
        → N이 지배적인 항이므로 P는 무시해도 된다 → O(N)
        
    - O(2N)
        
        → 상수항은 무시해도 된다 → O(N)
        
    - O(N + log N)
        
        → N이 지배적인 항이므로 log N은 무시해도 된다 → O(N)
        
    - O(N + M)
        
        → N과 M 사이에 어떤 연관 관계가 있는지 알 수 없기 때문에 두 변수 모두를 표시해주어야 한다 → O(N + M)
        

## 예제 8